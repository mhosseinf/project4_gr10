{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc1e6478-45c1-415d-a5c8-130fdda64653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mh30f\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import our dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3522d6cb-938a-4dc1-aac0-6552ff7932e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>stationCode</th>\n",
       "      <th>stationName</th>\n",
       "      <th>rainfall</th>\n",
       "      <th>relativeHumidity</th>\n",
       "      <th>airTemperature_avg</th>\n",
       "      <th>wind_avg_speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>AN001</td>\n",
       "      <td>Allanooka</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>17.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-02</td>\n",
       "      <td>AN001</td>\n",
       "      <td>Allanooka</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.2</td>\n",
       "      <td>25.2</td>\n",
       "      <td>16.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-03</td>\n",
       "      <td>AN001</td>\n",
       "      <td>Allanooka</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.1</td>\n",
       "      <td>32.2</td>\n",
       "      <td>24.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-04</td>\n",
       "      <td>AN001</td>\n",
       "      <td>Allanooka</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.9</td>\n",
       "      <td>32.1</td>\n",
       "      <td>21.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-05</td>\n",
       "      <td>AN001</td>\n",
       "      <td>Allanooka</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>33.3</td>\n",
       "      <td>18.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47384</th>\n",
       "      <td>2023-08-09</td>\n",
       "      <td>YU002</td>\n",
       "      <td>Yuna NE</td>\n",
       "      <td>0.2</td>\n",
       "      <td>74.2</td>\n",
       "      <td>14.2</td>\n",
       "      <td>5.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47385</th>\n",
       "      <td>2023-08-10</td>\n",
       "      <td>YU002</td>\n",
       "      <td>Yuna NE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>72.1</td>\n",
       "      <td>14.8</td>\n",
       "      <td>5.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47386</th>\n",
       "      <td>2023-08-11</td>\n",
       "      <td>YU002</td>\n",
       "      <td>Yuna NE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>72.8</td>\n",
       "      <td>16.8</td>\n",
       "      <td>6.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47387</th>\n",
       "      <td>2023-08-12</td>\n",
       "      <td>YU002</td>\n",
       "      <td>Yuna NE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75.7</td>\n",
       "      <td>15.9</td>\n",
       "      <td>7.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47388</th>\n",
       "      <td>2023-08-13</td>\n",
       "      <td>YU002</td>\n",
       "      <td>Yuna NE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>77.4</td>\n",
       "      <td>14.6</td>\n",
       "      <td>8.47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47389 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date stationCode stationName  rainfall  relativeHumidity  \\\n",
       "0      2023-01-01       AN001   Allanooka       0.0              66.1   \n",
       "1      2023-01-02       AN001   Allanooka       0.0              52.2   \n",
       "2      2023-01-03       AN001   Allanooka       0.0              19.1   \n",
       "3      2023-01-04       AN001   Allanooka       0.0              22.9   \n",
       "4      2023-01-05       AN001   Allanooka       0.0              17.8   \n",
       "...           ...         ...         ...       ...               ...   \n",
       "47384  2023-08-09       YU002     Yuna NE       0.2              74.2   \n",
       "47385  2023-08-10       YU002     Yuna NE       0.0              72.1   \n",
       "47386  2023-08-11       YU002     Yuna NE       0.0              72.8   \n",
       "47387  2023-08-12       YU002     Yuna NE       0.0              75.7   \n",
       "47388  2023-08-13       YU002     Yuna NE       0.0              77.4   \n",
       "\n",
       "       airTemperature_avg  wind_avg_speed  \n",
       "0                    21.0           17.93  \n",
       "1                    25.2           16.49  \n",
       "2                    32.2           24.77  \n",
       "3                    32.1           21.67  \n",
       "4                    33.3           18.12  \n",
       "...                   ...             ...  \n",
       "47384                14.2            5.72  \n",
       "47385                14.8            5.71  \n",
       "47386                16.8            6.62  \n",
       "47387                15.9            7.51  \n",
       "47388                14.6            8.47  \n",
       "\n",
       "[47389 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save rain_df into csv for optimisasation file\n",
    "file_path = '../../Resources/raindata.csv'\n",
    "\n",
    "# Export rain_df to CSV\n",
    "rain_df=pd.read_csv(file_path)\n",
    "rain_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57b607fd-0805-4dfd-b958-cc37f8f92f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our preprocessed data into our features and target arrays\n",
    "y = rain_df[\"rainfall\"].values\n",
    "X = rain_df.drop([\"rainfall\",\"stationName\"],axis=1).values\n",
    "\n",
    "# Split the preprocessed data into a training and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "766998e9-edd8-414e-a097-5b15844e9f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['2023-01-01', 'AN001', 66.1, 21.0, 17.93],\n",
       "       ['2023-01-02', 'AN001', 52.2, 25.2, 16.49],\n",
       "       ['2023-01-03', 'AN001', 19.1, 32.2, 24.77],\n",
       "       ...,\n",
       "       ['2023-08-11', 'YU002', 72.8, 16.8, 6.62],\n",
       "       ['2023-08-12', 'YU002', 75.7, 15.9, 7.51],\n",
       "       ['2023-08-13', 'YU002', 77.4, 14.6, 8.47]], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1481319c-99de-45b1-9930-2f1b5fa7e2f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '2023-01-30'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Fit the StandardScaler\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m X_scaler \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit(X_train)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Scale the data\u001b[39;00m\n\u001b[0;32m      8\u001b[0m X_train_scaled \u001b[38;5;241m=\u001b[39m X_scaler\u001b[38;5;241m.\u001b[39mtransform(X_train)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:837\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    835\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    836\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 837\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartial_fit(X, y, sample_weight)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:873\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    841\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[0;32m    842\u001b[0m \n\u001b[0;32m    843\u001b[0m \u001b[38;5;124;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    870\u001b[0m \u001b[38;5;124;03m    Fitted scaler.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    872\u001b[0m first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 873\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    874\u001b[0m     X,\n\u001b[0;32m    875\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    876\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mFLOAT_DTYPES,\n\u001b[0;32m    877\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    878\u001b[0m     reset\u001b[38;5;241m=\u001b[39mfirst_call,\n\u001b[0;32m    879\u001b[0m )\n\u001b[0;32m    880\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    882\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:604\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    602\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 604\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    606\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:917\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    915\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    916\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 917\u001b[0m         array \u001b[38;5;241m=\u001b[39m _asarray_with_order(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    920\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    921\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:380\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    378\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 380\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39masarray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    382\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: '2023-01-30'"
     ]
    }
   ],
   "source": [
    "# Create a StandardScaler instances\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a013e881-43f7-4e88-9b1d-a58c731b673f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a method that creates a new Sequential model with hyperparameter options\n",
    "def create_model(hp):\n",
    "    nn_model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Allow kerastuner to decide which activation function to use in hidden layers\n",
    "    activation = hp.Choice('activation', ['relu', 'tanh', 'sigmoid'])\n",
    "    \n",
    "    # Allow kerastuner to decide number of neurons in the first layer\n",
    "    nn_model.add(tf.keras.layers.Dense(units=hp.Int('first_units',\n",
    "        min_value=64,\n",
    "        max_value=512,\n",
    "        step=2), activation=activation, input_dim=len(X_train_scaled[0])))\n",
    "\n",
    "    # Allow kerastuner to decide number of hidden layers and neurons in hidden layers\n",
    "    for i in range(hp.Int('num_layers', 1, 20)):\n",
    "        nn_model.add(tf.keras.layers.Dense(units=hp.Int('units_' + str(i),\n",
    "            min_value=64,\n",
    "            max_value=256,\n",
    "            step=2),\n",
    "            activation=activation))\n",
    "    \n",
    "    nn_model.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "    \n",
    "    # Compile the model\n",
    "    nn_model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n",
    "    \n",
    "    return nn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdbb43a-cb82-4600-acdb-393cf0655719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Keras Tuner\n",
    "tuner = kt.Hyperband(\n",
    "    create_model,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_epochs=100,\n",
    "    hyperband_iterations=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1df7dd0-3550-458a-b41d-39820dc4b20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a callback to stop training early if there's no improvement in validation accuracy\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3353b2fb-ffed-4b5e-9832-aba94043ae90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the kerastuner search for best hyperparameters\n",
    "# tuner.search(X_train_scaled,y_train,epochs=50,validation_data=(X_test_scaled,y_test))\n",
    "tuner.search(X_train_scaled, y_train, epochs=50, validation_split=0.2, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6361fbe0-1320-44f8-bde6-dab62c8e6e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model hyperparameters\n",
    "best_hyper = tuner.get_best_hyperparameters(1)[0]\n",
    "# Build the model with the best hyperparameters and train it on the data\n",
    "model = tuner.hypermodel.build(best_hyper)\n",
    "# Display the summary of the best model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2efbaa-2324-4e9b-8140-4831f1deaef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_scaled, y_train, epochs=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12befb58-7e17-4a43-a143-fbdd7c1f8bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = model.evaluate(X_test_scaled, y_test, verbose=2)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7f2541-5762-4809-b389-52ebb72bbe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=78)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test_scaled)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "# Print the Random Forest evaluation results\n",
    "print(f\"Random Forest Accuracy: {rf_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5715df78-45c6-4c85-aebb-586681181dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Decision Tree model\n",
    "decision_tree_model = DecisionTreeClassifier(random_state=78)\n",
    "\n",
    "# Fit the model to the training data\n",
    "decision_tree_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = decision_tree_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the model accuracy\n",
    "accuracy_dt = accuracy_score(y_test, y_pred)\n",
    "# Print the accuracy\n",
    "print(f\"Decision Tree Model Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85c34a9-5ac2-4f38-ae84-fe70bfbe6b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare accuracies and select the final model\n",
    "if model_accuracy > accuracy_dt and model_accuracy > accuracy_rf:\n",
    "    final_model = model\n",
    "    print(\"Using Neural Network as the final model.\")\n",
    "elif accuracy_dt > accuracy_rf:\n",
    "    final_model = decision_tree_model\n",
    "    print(\"Using Decision Tree as the final model.\")\n",
    "else:\n",
    "    final_model = random_forest_model\n",
    "    print(\"Using Random Forest as the final model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df12a208-0bed-4cbf-8c4b-8d47f4afe4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export our model to HDF5 file\n",
    "final_model.save(\"AlphabetSoupCharity_Optimisation.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3f7cb7-641f-4950-8323-f61ac7fa6a06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
